{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a5b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing steps\n",
    "# Load Nepali stopwords\n",
    "# You may need to download a Nepali stopword list\n",
    "import spacy\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Extract stop words for nepali texts\n",
    "nepali_stopwords = []\n",
    "with open('dataset/non-potential-topic-word-list.txt', 'r', encoding='utf-8') as f:\n",
    "    nepali_stopwords = [line.strip() for line in f]  # Use a set for efficient lookup\n",
    "    \n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# Combine with built-in English stopwords\n",
    "combined_stopwords = list(text.ENGLISH_STOP_WORDS.union(nepali_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca66255",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/docs_saved_new.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m output_dir = \u001b[33m'\u001b[39m\u001b[33mresults/LDA/\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Open and read the content of data.txt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset/docs_saved_new.txt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m      5\u001b[39m     content = file.read()\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# If the content is already a valid JSON list, just load it\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'dataset/docs_saved_new.txt'"
     ]
    }
   ],
   "source": [
    "output_dir = 'results/LDA/'\n",
    "\n",
    "# Open and read the content of data.txt\n",
    "with open('dataset/docs_saved_new.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# If the content is already a valid JSON list, just load it\n",
    "import json\n",
    "\n",
    "try:\n",
    "    docs = json.loads(content)\n",
    "except json.JSONDecodeError:\n",
    "    # If not, try to manually split paragraphs (fallback)\n",
    "    docs = [para.strip() for para in content.split('\\n') if para.strip()]\n",
    "\n",
    "# Print the list or process it further\n",
    "print(docs[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16172611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "import nltk\n",
    "\n",
    "\n",
    "# Tokenize and clean\n",
    "texts = [\n",
    "    [word for word in doc.split() if word not in combined_stopwords and len(word) > 2]\n",
    "    for doc in docs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21963459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Filter extremes (optional, tuneable)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879a6963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=50,          # tune this\n",
    "    random_state=100,\n",
    "    chunksize=200,          # larger chunks stabilize learning\n",
    "    passes=20,              # increase passes for better convergence\n",
    "    alpha='symmetric',      # or 'auto' for self-tuning\n",
    "    eta='auto',             # improves word-topic distribution balance\n",
    "    per_word_topics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cfa8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 23: 0.000*\"भवितव्य\" + 0.000*\"मुसलमान\" + 0.000*\"जगाउने\" + 0.000*\"झुठ्ठा\" + 0.000*\"पूर्वरिसइवी\" + 0.000*\"फर्किआए\" + 0.000*\"मुक्\" + 0.000*\"एकआपस\" + 0.000*\"हिर्काए\" + 0.000*\"भूमिहार\"\n",
      "Topic 90: 0.000*\"भवितव्य\" + 0.000*\"मुसलमान\" + 0.000*\"जगाउने\" + 0.000*\"झुठ्ठा\" + 0.000*\"पूर्वरिसइवी\" + 0.000*\"फर्किआए\" + 0.000*\"मुक्\" + 0.000*\"एकआपस\" + 0.000*\"हिर्काए\" + 0.000*\"भूमिहार\"\n",
      "Topic 2: 0.000*\"भवितव्य\" + 0.000*\"मुसलमान\" + 0.000*\"जगाउने\" + 0.000*\"झुठ्ठा\" + 0.000*\"पूर्वरिसइवी\" + 0.000*\"फर्किआए\" + 0.000*\"मुक्\" + 0.000*\"एकआपस\" + 0.000*\"हिर्काए\" + 0.000*\"भूमिहार\"\n",
      "Topic 5: 0.000*\"भवितव्य\" + 0.000*\"मुसलमान\" + 0.000*\"जगाउने\" + 0.000*\"झुठ्ठा\" + 0.000*\"पूर्वरिसइवी\" + 0.000*\"फर्किआए\" + 0.000*\"मुक्\" + 0.000*\"एकआपस\" + 0.000*\"हिर्काए\" + 0.000*\"भूमिहार\"\n",
      "Topic 9: 0.000*\"भवितव्य\" + 0.000*\"मुसलमान\" + 0.000*\"जगाउने\" + 0.000*\"झुठ्ठा\" + 0.000*\"पूर्वरिसइवी\" + 0.000*\"फर्किआए\" + 0.000*\"मुक्\" + 0.000*\"एकआपस\" + 0.000*\"हिर्काए\" + 0.000*\"भूमिहार\"\n",
      "Topic 56: 0.000*\"भवितव्य\" + 0.000*\"मुसलमान\" + 0.000*\"जगाउने\" + 0.000*\"झुठ्ठा\" + 0.000*\"पूर्वरिसइवी\" + 0.000*\"फर्किआए\" + 0.000*\"मुक्\" + 0.000*\"एकआपस\" + 0.000*\"हिर्काए\" + 0.000*\"भूमिहार\"\n",
      "Topic 16: 0.000*\"भवितव्य\" + 0.000*\"मुसलमान\" + 0.000*\"जगाउने\" + 0.000*\"झुठ्ठा\" + 0.000*\"पूर्वरिसइवी\" + 0.000*\"फर्किआए\" + 0.000*\"मुक्\" + 0.000*\"एकआपस\" + 0.000*\"हिर्काए\" + 0.000*\"भूमिहार\"\n",
      "Topic 41: 0.000*\"भवितव्य\" + 0.000*\"मुसलमान\" + 0.000*\"जगाउने\" + 0.000*\"झुठ्ठा\" + 0.000*\"पूर्वरिसइवी\" + 0.000*\"फर्किआए\" + 0.000*\"मुक्\" + 0.000*\"एकआपस\" + 0.000*\"हिर्काए\" + 0.000*\"भूमिहार\"\n",
      "Topic 35: 0.000*\"भवितव्य\" + 0.000*\"मुसलमान\" + 0.000*\"जगाउने\" + 0.000*\"झुठ्ठा\" + 0.000*\"पूर्वरिसइवी\" + 0.000*\"फर्किआए\" + 0.000*\"मुक्\" + 0.000*\"एकआपस\" + 0.000*\"हिर्काए\" + 0.000*\"भूमिहार\"\n",
      "Topic 33: 0.000*\"भवितव्य\" + 0.000*\"मुसलमान\" + 0.000*\"जगाउने\" + 0.000*\"झुठ्ठा\" + 0.000*\"पूर्वरिसइवी\" + 0.000*\"फर्किआए\" + 0.000*\"मुक्\" + 0.000*\"एकआपस\" + 0.000*\"हिर्काए\" + 0.000*\"भूमिहार\"\n",
      "Topic 40: 0.034*\"पक्ष\" + 0.033*\"विषय\" + 0.030*\"कानूनी\" + 0.029*\"सिद्धान्त\" + 0.029*\"कायम\" + 0.028*\"मान्यता\" + 0.028*\"स्थिति\" + 0.027*\"प्रश्न\" + 0.024*\"सम्बन्धित\" + 0.021*\"सन्दर्भ\"\n",
      "Topic 70: 0.084*\"कानून\" + 0.053*\"प्रकरण\" + 0.052*\"सरकार\" + 0.027*\"कानूनी\" + 0.023*\"लागू\" + 0.022*\"कार्यान्वयन\" + 0.021*\"आवश्यक\" + 0.020*\"निर्धारण\" + 0.017*\"प्रावधान\" + 0.017*\"जारी\"\n",
      "Topic 58: 0.110*\"व्यवहार\" + 0.058*\"मानसिक\" + 0.053*\"यातना\" + 0.052*\"शारीरिक\" + 0.034*\"दुर्व्यवहार\" + 0.029*\"बाध्य\" + 0.024*\"अंग\" + 0.020*\"विभिन्न\" + 0.019*\"ग्नें\" + 0.017*\"गर्दे\"\n",
      "Topic 22: 0.131*\"वमोजिम\" + 0.097*\"दावी\" + 0.087*\"पेश\" + 0.058*\"पाउँ\" + 0.055*\"सरकार\" + 0.031*\"कानून\" + 0.027*\"देखिंदा\" + 0.026*\"उल्लेखित\" + 0.025*\"मागदावी\" + 0.025*\"कार्यविधि\"\n",
      "Topic 19: 0.133*\"बयान\" + 0.132*\"वारदात\" + 0.108*\"अभियोग\" + 0.098*\"जाहेरी\" + 0.037*\"पुष्टि\" + 0.036*\"यौन\" + 0.032*\"कागज\" + 0.030*\"अनुसन्धान\" + 0.028*\"दावी\" + 0.028*\"सफाई\"\n",
      "Topic 59: 0.160*\"प्रहरी\" + 0.103*\"मुचुल्\" + 0.073*\"घटनास्थल\" + 0.058*\"वडा\" + 0.049*\"जाहेरी\" + 0.049*\"बयान\" + 0.034*\"प्रतिवेदन\" + 0.033*\"खोज्दा\" + 0.032*\"खान\" + 0.032*\"महानगरपालि\"\n",
      "Topic 31: 0.174*\"कागज\" + 0.164*\"घटना\" + 0.105*\"विवरण\" + 0.057*\"मानिस\" + 0.056*\"मृत्यु\" + 0.029*\"फेला\" + 0.029*\"ढो\" + 0.022*\"भनेपछि\" + 0.022*\"सुनी\" + 0.019*\"जानकारी\"\n",
      "Topic 96: 0.067*\"मोबाइल\" + 0.051*\"हात\" + 0.036*\"लिई\" + 0.034*\"उत्तर\" + 0.030*\"पश्चिम\" + 0.029*\"दक्षिण\" + 0.029*\"पैसा\" + 0.020*\"देखी\" + 0.018*\"बसे\" + 0.018*\"नदी\"\n",
      "Topic 15: 0.059*\"बाबु\" + 0.052*\"लिखत\" + 0.041*\"बच्चा\" + 0.038*\"पाऊँ\" + 0.028*\"सहमति\" + 0.027*\"ग्नें\" + 0.027*\"ने\" + 0.025*\"प्राप्त\" + 0.019*\"सन्तान\" + 0.017*\"सम्पूर्ण\"\n",
      "Topic 72: 0.082*\"प्रमाण\" + 0.062*\"बेहोरा\" + 0.037*\"कसुर\" + 0.031*\"प्रमाणित\" + 0.023*\"संलग्न\" + 0.023*\"मिसिल\" + 0.022*\"उल्लिखित\" + 0.018*\"अर्\" + 0.016*\"विचार\" + 0.015*\"देखिदैन\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print top 10 words from each topic\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "for i, topic in topics:\n",
    "    print(f\"Topic {i}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0331f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model_lda = CoherenceModel(\n",
    "    model=lda_model,\n",
    "    texts=texts,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb33de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "topic_words = []\n",
    "for i, topic in lda_model.show_topics( formatted=False):\n",
    "    words = [word for word, prob in topic]\n",
    "    topic_words.append({\"Topic\": i, \"Words\": \", \".join(words)})\n",
    "\n",
    "df = pd.DataFrame(topic_words)\n",
    "df.to_csv(output_dir + \"lda_topics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_topic_diversity(lda_model, top_k=10):\n",
    "    topic_words = lda_model.show_topics(num_topics=-1, num_words=top_k, formatted=False)\n",
    "\n",
    "    all_words = []\n",
    "    for topic in topic_words:\n",
    "        words = [word for word, _ in topic[1]]\n",
    "        all_words.extend(words)\n",
    "\n",
    "    unique_words = set(all_words)\n",
    "    diversity_score = len(unique_words) / len(all_words)\n",
    "    return diversity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a89cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.4302\n",
      "Topic Diversity: 0.5710\n"
     ]
    }
   ],
   "source": [
    "\n",
    "coherence_score = coherence_model_lda.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score:.4f}\")\n",
    "\n",
    "diversity = calculate_topic_diversity(lda_model, top_k=10)\n",
    "print(f\"Topic Diversity: {diversity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6279fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23,\n",
       " '0.000*\"भवितव्य\" + 0.000*\"मुसलमान\" + 0.000*\"जगाउने\" + 0.000*\"झुठ्ठा\" + 0.000*\"पूर्वरिसइवी\" + 0.000*\"फर्किआए\" + 0.000*\"मुक्\" + 0.000*\"एकआपस\" + 0.000*\"हिर्काए\" + 0.000*\"भूमिहार\"')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
